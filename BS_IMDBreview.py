# -*- coding: utf-8 -*-
"""nlp_imdb_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_k3l_bJ9lj6wx0c8D_ZPQaoe0ZTuHRbC
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup

# Set base url and page number
url = "https://www.imdb.com/title/tt1825683/reviews?ref_=tt_urv"
page_num = 1

# Initialize empty list to store review data
reviews = []

# Iterate through pages 
for i in range(1, 6):
    # Send request to url and get HTML content
    response = requests.get(url)
    content = response.content

    # Parse HTML content with BeautifulSoup
    soup = BeautifulSoup(content, 'html.parser')

    # Find all review divs on the page
    review_divs = soup.find_all('div', class_='lister-item-content')

    # Iterate through each review div
    for review_div in review_divs:
        # Get review title
        title = review_div.find('a', class_='title').text

        # Get review score
        score = review_div.find('span', class_='rating-other-user-rating').text

        # Get review content
        content = review_div.find('div', class_='text show-more__control').text

        # Create dictionary for review data
        review = {
            'title': title,
            'score': score,
            'content': content
        }

        # Append review data to reviews list
        reviews.append(review)

    # Update page number and url for next iteration
    page_num += 1
    url = f"https://www.imdb.com/title/tt1825683/reviews?start={(page_num-1)*10}&ref_=tt_urv"

# Create dataframe from reviews list
df = pd.DataFrame(reviews)

# Save dataframe to CSV file
df.to_csv('reviews.csv', index=False)